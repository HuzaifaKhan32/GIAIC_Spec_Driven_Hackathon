Feature: RAG Chatbot Backend - Question Answering System Intent: Answer user questions from textbook using Gemini 2.5 Flash Framework: FastAPI (Python 3.10+) LLM: Gemini 2.5 Flash (free tier) Vector DB: Qdrant Cloud (free tier) Embeddings: Google Gemini Embeddings API Architecture: User Query → FastAPI → Gemini embeddings → Qdrant search → Gemini LLM → Cited response Core Endpoints: POST /chat/query - Input: {query, session_id, selected_text?} - Output: {response, sources: [{chapter, section}], confidence} - Latency: < 2 seconds POST /documents/process - Process book chapters into Qdrant GET /chat/history - Retrieve conversation history GET /health - Verify all services (Gemini, Qdrant, FastAPI) RAG Pipeline: - Chunk size: 500-1000 tokens per chunk - Metadata: {chapter_id, section, index, full_content} - Embedding model: Google Gemini (text-embedding-004) - Similarity: Top-5 results per query - System prompt: "Answer about Physical AI & Humanoid Robotics textbook. Cite sources only." - Session context: Last 10 messages, max 8000 tokens Security: - Rate limiting: 100 requests/minute per IP - CORS: Allow Vercel domain only - Session-based auth (Better-Auth cookies) Testing: - User asks "What is Physical AI?" → Gets chapter 1 answer with citations - Selected text queries work correctly - Follow-up questions use context - Error handling graceful Acceptance Criteria: - Server starts without errors - All endpoints return 2xx responses - Chat responses within 2 seconds - Citations accurate (chapter/section) - Qdrant properly vectorized - Error handling > 85% accuracy Reference: constitution.md Section 4.2, 7.2